# Data-Lake-Using-Spark
Developed an ETL pipeline for a Data Lake that extracts data from S3, processes the data using Spark, and loads the data back into S3 as a set of dimensional tables in form of Parquet files. Lake Processing: Spark, Lake Storage: S3
